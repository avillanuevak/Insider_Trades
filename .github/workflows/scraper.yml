name: Run Scraper

on:
  schedule:
    # Run every hour during market hours (UTC)
    # US ET: 9:30 AM - 4:00 PM = UTC 13:30 - 20:00
    - cron: '0 13-20 * * 1-5'
  workflow_dispatch:  # Allows manual trigger from GitHub UI

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v2
      with:
        fetch-depth: 0  # Fetch all history for all branches and tags
      
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.x'
        
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run scraper
      run: |
        python Scraping/Scrape_Info.py
      
    - name: Commit and push if changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add -A
        git diff --quiet && git diff --staged --quiet || (git commit -m "Update scraped data" && git push) 